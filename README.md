# Explainable AI for Drug Discovery and Analysis

## Overview
This project employs **Machine Learning (ML) and Deep Learning (DL)** techniques to predict drug toxicity and ensure interpretability through **Explainable AI (XAI)** approaches such as **SHapley Additive exPlanations (SHAP)** and **Local Interpretable Model-Agnostic Explanations (LIME)**. The goal is to improve trust in AI-based drug analysis by providing transparent model predictions.

## Features
- **Drug Toxicity Prediction:** Uses Random Forest and Graph Neural Networks (GNNs) to classify drug reactions.
- **Data Preprocessing:** Handles missing values, encodes labels, and applies **Principal Component Analysis (PCA)** for dimensionality reduction.
- **Explainability with SHAP & LIME:** Enhances model transparency by explaining individual predictions.
- **Performance Evaluation:** Assesses accuracy, precision, recall, F1-score, and execution time.
- **Visualization Tools:** Generates validation graphs, comparison charts, and node embedding graphs.

## Technologies Used
- **Programming Languages:** Python
- **Frameworks & Libraries:**
  - Machine Learning: Scikit-learn, TensorFlow, PyTorch
  - Explainability: SHAP, LIME
  - Data Processing: Pandas, NumPy, Scipy
  - Visualization: Matplotlib, Seaborn
  - Web App: Streamlit
- **Development Tools:** Anaconda Navigator, Spyder IDE

  

## Usage
- Upload the **drug discovery dataset** (CSV format).
- The system will **preprocess** and classify drug toxicity using ML/DL models.
- **SHAP and LIME visualizations** will provide explanations for model predictions.
- Results will be displayed with performance metrics and comparison charts.

## Dataset
- The dataset consists of **~3,000 rows and 15 features**, including drug name, category, pregnancy category, and side effects.

## Contributing
Contributions are welcome! Feel free to submit issues or pull requests to enhance the project.


## Contact
For any queries, reach out to: 123koushiksiddu123@gmail.com

